
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <title>面试总结 | hello world ~~!</title>
    <meta name="description"
        content="xunfei - 自动驾驶
基本没做过相关工作，yolov11s也就是用的ultralytics, 没有怎么研究
唯一的收获就是他问我解决过什么技术难点，这块回答的太差了，明明做过很多工作，但是面试的时候啥也没想到，脑子简直蒙了
位姿估计


我们试过目前最先进的方法了： 我们最初想做姿态估计的时候，肯定也是找目前的SOTA方法，就把注意力转向了foundationPose，， FoundationPose不仅有开源代码，还有在jetson平台上的Issac ROS Nitros加速代码，用的tensorrt 部署和GXF图框架，很符合我们的项目需求，所以我们就开始研究foundationPose.
先看pytorch源码，然后复现结果。foundationpose 其实方法特别暴力， 并且他只能做单目标的位姿估计，同时需要rgb, 深度图，合适的mesh（物体size不能差距太大）， 原论文采用了Nerf生成mesh的方法，但是Nerf重建方法对于小物体的精度不够，
我们发现Isaac Ros里面的Foundation Pose 的位置估计初始总是有偏差，然后导致随后的跟踪效果越来越差， 然后深刻理解他的原理之后， 我开始对GXF框架编写的FoundationPose代码进行debug, 然后发现是他们部署的代码有bug,  还有跟踪发出的时间戳保持了初始时刻姿态估计的时间戳，这是不符合实际项目的需求的，我们修复了他的源代码，满足了实际项目需求。
因为GXF 是nvidia 内部使用的框架，所以网上公开的资料很少，开始我们只能通过点点资料 &#43; 断点调试 &#43; 猜测去分析整个项目，当时感觉很难很难继续往下推进，但是靠自己的努力，一点点的把bug啃了下来，后来和NV交流的时候他们也直接建议我们不要去改他们GXF写的源码， 但是确实是有bug
做出目标估计我们需要验证精度，验证精度我们又是发难了，因为验证精度们，你总要找个能测出姿态的更精确的传感器吧，但是我们部门的支持也不够，所以我们也只能想其他办法，那怎么办呢，我们做视觉的，那只能想视觉的办法了。所以就用了Aruco标定版，solvepnp的方式去求解位姿。
结果方向foundation pose的姿态转化为欧拉角之后，和标定出来的有几度的差距，精度是达不到要求，foundationpose已经是深度学习最先进的方法了，我们的目标还是一个无纹理的物体，所以用colmap估计真实位姿也挺难，没办法制作数据集，learning的方法就不考虑了，只能想其他办法了。后来我们就把目标就投到了传统方法，就尝试用目标检测扣点云，ransac法向量估计，然后再将inlier点投影到平面上，去求最小外接转矩形，再把求的的方向反变换回来，得到物体真实姿态，然后发现传统方法确实要比foundation pose的效果好，就在我们的任务中用了最合适的传统方法。


点云孔洞处理

我们的插拔typeC操作需要近距离操作，所以最开始我们用了D405,因为官方文档不是说工具距离更近吗， 但是405是个双目，他对于无纹理和镜面深度估计不行，全是孔洞，最开始就是自己在物体背面涂画一些涂鸦，人为制造纹理，然后去打光，制造对比度，发现点云效果还行。但是实际产线肯定不让你涂东西的/
后来我们只能改成435, 因为他是结构光 &#43; 双目，所以对于无纹理的可以很好的估计，但是我们发现效果有时候还是不行，点云时好时坏的，还是会出现大面积的孔洞。想尝试加偏振片，但是领导也不给予支持，所以也没办法。后来只能去开启滤波，包括孔洞填充，空间滤波，去让点云变得稳定。
但是滤波也不能解决核心问题，还是会有孔洞，因为我们从405的经验是打光能让点云变好，因为我们的物体是iphone手机么，两面都是高光材质，所以有镜面反射，所以我们猜测是不是镜面反射让结构光失效了，于是我们就把环境光调低，果然点云效果就变好了，然后解决了这个问题。

UV固化，一个月演示


时间特别紧，所有感知相关的活都要自己干，而且还要去深圳产线现场连调，一个月项目就要演示。


以前标数据都是阿里云去标，但是这次时间太紧了，阿里云标注时间特别长，所以我们就想借助大模型做工具，于是就去找了Grouded sam2,   利用他完成了5000张数据集的标注，利用ultralytics Yolov11进行训练，用BotSort完成多目标跟踪，3天实现产线上pad的稳定分割和追踪


工业相机的标定，一直不给买大的标定版，我们只能用A4纸打印的去标定，但是工业相机架的很高，所以手在眼外标定的精度很差，所以我们只能想其他迂回的办法了。 所幸的是，我们手臂上还有一个摄像头，我们手臂上眼在手上标定因为距离近，还是可以得到一个不错的精度的，用这个间接去算工业相机的标定。我们把标定版抬高，然后机械臂也抬高，知道标定版相对于工业相机关系，也知道标定版相对于手臂上realsense的关系，因为realsense的眼在手上有，这样就可以间接得到工业相机在baselink下的关系，最后为了结果稳定，我们将多次不同位置的结果做了平均。


最后pad放进固化炉子之后，因为需要用视觉方法确定其是否烟丝合缝的嵌入进去了，我们想的方法就是先走到固定点位，然后获取凹进去的平面的mask, 然后pad放进去之后，做完实例分割之后，二者求交，大于阈值就判断放好了，如果没有还需要二次调整，但是因为我们自研的机械臂重复定位精度差，沒办法每次走到相同点位，而且随着运行时间长越走越差，所以这种方法就失效了，为了能够做判断，我们只能是将相机固定在机械臂外，这样每次的凹进去的mask真实值就不会再变了。


yolov11 架构能讲解一下吗？但是我还是应该了解一下最后是怎么检测的

从我搞科研的经历说起，改模型是一项吃力不讨好的工作，收效甚微，对于我们做业务的人来说，能够快速上线，稳定检测是紧急的任务，所以我并不会优先去做改模型的工作。
我们的场景很单一，目标也比较单一, 所以我们更希望的是能够让灰度和彩色图像共用一套代码，我们对于数据预处理和数据增强做了些改进，比方说把灰度图叠加三个通道，使得其能和彩色图像一块训，然后比方说对于彩色图像数据，我们要随机让他转化为灰度图像，这样子去增强灰度图像检测效果。

VLA模型遇到的问题

末端姿态控制，导致经常进去奇异点。如果你判断他不懂了，你就要自己去做些短程的运动控制，去从奇异点走出来。
你好,.明天

Flow matching的原理

速度场建模，基于最速传输
初始化一个随即噪声和时间t,  然后在基于时间t在样本和噪声之间做插值，样本和噪声之间的最快速度就是应该二者的差值，去噪声过程就是去预测这两个差值，

CVAE的原理- ACT的原理


encoder 基于observation 和 action 去预测隐变量的均值和方差">
    <link rel="canonical" href="https://summertaiyuan.github.io/post/title/" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css">
    
    <link rel="stylesheet" href="https://summertaiyuan.github.io/scss/style.min.aa7209164f013c6883aa54384ea45a46e582eacf9a17e9565badebd557dd1059.css">

    <meta property="og:url" content="https://summertaiyuan.github.io/post/title/">
  <meta property="og:site_name" content="hello world ~~!">
  <meta property="og:title" content="面试总结">
  <meta property="og:description" content="xunfei - 自动驾驶 基本没做过相关工作，yolov11s也就是用的ultralytics, 没有怎么研究
唯一的收获就是他问我解决过什么技术难点，这块回答的太差了，明明做过很多工作，但是面试的时候啥也没想到，脑子简直蒙了
位姿估计 我们试过目前最先进的方法了： 我们最初想做姿态估计的时候，肯定也是找目前的SOTA方法，就把注意力转向了foundationPose，， FoundationPose不仅有开源代码，还有在jetson平台上的Issac ROS Nitros加速代码，用的tensorrt 部署和GXF图框架，很符合我们的项目需求，所以我们就开始研究foundationPose.
先看pytorch源码，然后复现结果。foundationpose 其实方法特别暴力， 并且他只能做单目标的位姿估计，同时需要rgb, 深度图，合适的mesh（物体size不能差距太大）， 原论文采用了Nerf生成mesh的方法，但是Nerf重建方法对于小物体的精度不够，
我们发现Isaac Ros里面的Foundation Pose 的位置估计初始总是有偏差，然后导致随后的跟踪效果越来越差， 然后深刻理解他的原理之后， 我开始对GXF框架编写的FoundationPose代码进行debug, 然后发现是他们部署的代码有bug, 还有跟踪发出的时间戳保持了初始时刻姿态估计的时间戳，这是不符合实际项目的需求的，我们修复了他的源代码，满足了实际项目需求。
因为GXF 是nvidia 内部使用的框架，所以网上公开的资料很少，开始我们只能通过点点资料 &#43; 断点调试 &#43; 猜测去分析整个项目，当时感觉很难很难继续往下推进，但是靠自己的努力，一点点的把bug啃了下来，后来和NV交流的时候他们也直接建议我们不要去改他们GXF写的源码， 但是确实是有bug
做出目标估计我们需要验证精度，验证精度我们又是发难了，因为验证精度们，你总要找个能测出姿态的更精确的传感器吧，但是我们部门的支持也不够，所以我们也只能想其他办法，那怎么办呢，我们做视觉的，那只能想视觉的办法了。所以就用了Aruco标定版，solvepnp的方式去求解位姿。
结果方向foundation pose的姿态转化为欧拉角之后，和标定出来的有几度的差距，精度是达不到要求，foundationpose已经是深度学习最先进的方法了，我们的目标还是一个无纹理的物体，所以用colmap估计真实位姿也挺难，没办法制作数据集，learning的方法就不考虑了，只能想其他办法了。后来我们就把目标就投到了传统方法，就尝试用目标检测扣点云，ransac法向量估计，然后再将inlier点投影到平面上，去求最小外接转矩形，再把求的的方向反变换回来，得到物体真实姿态，然后发现传统方法确实要比foundation pose的效果好，就在我们的任务中用了最合适的传统方法。
点云孔洞处理 我们的插拔typeC操作需要近距离操作，所以最开始我们用了D405,因为官方文档不是说工具距离更近吗， 但是405是个双目，他对于无纹理和镜面深度估计不行，全是孔洞，最开始就是自己在物体背面涂画一些涂鸦，人为制造纹理，然后去打光，制造对比度，发现点云效果还行。但是实际产线肯定不让你涂东西的/ 后来我们只能改成435, 因为他是结构光 &#43; 双目，所以对于无纹理的可以很好的估计，但是我们发现效果有时候还是不行，点云时好时坏的，还是会出现大面积的孔洞。想尝试加偏振片，但是领导也不给予支持，所以也没办法。后来只能去开启滤波，包括孔洞填充，空间滤波，去让点云变得稳定。 但是滤波也不能解决核心问题，还是会有孔洞，因为我们从405的经验是打光能让点云变好，因为我们的物体是iphone手机么，两面都是高光材质，所以有镜面反射，所以我们猜测是不是镜面反射让结构光失效了，于是我们就把环境光调低，果然点云效果就变好了，然后解决了这个问题。 UV固化，一个月演示 时间特别紧，所有感知相关的活都要自己干，而且还要去深圳产线现场连调，一个月项目就要演示。
以前标数据都是阿里云去标，但是这次时间太紧了，阿里云标注时间特别长，所以我们就想借助大模型做工具，于是就去找了Grouded sam2, 利用他完成了5000张数据集的标注，利用ultralytics Yolov11进行训练，用BotSort完成多目标跟踪，3天实现产线上pad的稳定分割和追踪
工业相机的标定，一直不给买大的标定版，我们只能用A4纸打印的去标定，但是工业相机架的很高，所以手在眼外标定的精度很差，所以我们只能想其他迂回的办法了。 所幸的是，我们手臂上还有一个摄像头，我们手臂上眼在手上标定因为距离近，还是可以得到一个不错的精度的，用这个间接去算工业相机的标定。我们把标定版抬高，然后机械臂也抬高，知道标定版相对于工业相机关系，也知道标定版相对于手臂上realsense的关系，因为realsense的眼在手上有，这样就可以间接得到工业相机在baselink下的关系，最后为了结果稳定，我们将多次不同位置的结果做了平均。
最后pad放进固化炉子之后，因为需要用视觉方法确定其是否烟丝合缝的嵌入进去了，我们想的方法就是先走到固定点位，然后获取凹进去的平面的mask, 然后pad放进去之后，做完实例分割之后，二者求交，大于阈值就判断放好了，如果没有还需要二次调整，但是因为我们自研的机械臂重复定位精度差，沒办法每次走到相同点位，而且随着运行时间长越走越差，所以这种方法就失效了，为了能够做判断，我们只能是将相机固定在机械臂外，这样每次的凹进去的mask真实值就不会再变了。
yolov11 架构能讲解一下吗？但是我还是应该了解一下最后是怎么检测的
从我搞科研的经历说起，改模型是一项吃力不讨好的工作，收效甚微，对于我们做业务的人来说，能够快速上线，稳定检测是紧急的任务，所以我并不会优先去做改模型的工作。
我们的场景很单一，目标也比较单一, 所以我们更希望的是能够让灰度和彩色图像共用一套代码，我们对于数据预处理和数据增强做了些改进，比方说把灰度图叠加三个通道，使得其能和彩色图像一块训，然后比方说对于彩色图像数据，我们要随机让他转化为灰度图像，这样子去增强灰度图像检测效果。
VLA模型遇到的问题 末端姿态控制，导致经常进去奇异点。如果你判断他不懂了，你就要自己去做些短程的运动控制，去从奇异点走出来。 你好,.明天 Flow matching的原理 速度场建模，基于最速传输 初始化一个随即噪声和时间t, 然后在基于时间t在样本和噪声之间做插值，样本和噪声之间的最快速度就是应该二者的差值，去噪声过程就是去预测这两个差值， CVAE的原理- ACT的原理 encoder 基于observation 和 action 去预测隐变量的均值和方差">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-01-10T00:50:32+08:00">
    <meta property="article:modified_time" content="2025-01-10T00:50:32+08:00">

    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="面试总结">
  <meta name="twitter:description" content="xunfei - 自动驾驶 基本没做过相关工作，yolov11s也就是用的ultralytics, 没有怎么研究
唯一的收获就是他问我解决过什么技术难点，这块回答的太差了，明明做过很多工作，但是面试的时候啥也没想到，脑子简直蒙了
位姿估计 我们试过目前最先进的方法了： 我们最初想做姿态估计的时候，肯定也是找目前的SOTA方法，就把注意力转向了foundationPose，， FoundationPose不仅有开源代码，还有在jetson平台上的Issac ROS Nitros加速代码，用的tensorrt 部署和GXF图框架，很符合我们的项目需求，所以我们就开始研究foundationPose.
先看pytorch源码，然后复现结果。foundationpose 其实方法特别暴力， 并且他只能做单目标的位姿估计，同时需要rgb, 深度图，合适的mesh（物体size不能差距太大）， 原论文采用了Nerf生成mesh的方法，但是Nerf重建方法对于小物体的精度不够，
我们发现Isaac Ros里面的Foundation Pose 的位置估计初始总是有偏差，然后导致随后的跟踪效果越来越差， 然后深刻理解他的原理之后， 我开始对GXF框架编写的FoundationPose代码进行debug, 然后发现是他们部署的代码有bug, 还有跟踪发出的时间戳保持了初始时刻姿态估计的时间戳，这是不符合实际项目的需求的，我们修复了他的源代码，满足了实际项目需求。
因为GXF 是nvidia 内部使用的框架，所以网上公开的资料很少，开始我们只能通过点点资料 &#43; 断点调试 &#43; 猜测去分析整个项目，当时感觉很难很难继续往下推进，但是靠自己的努力，一点点的把bug啃了下来，后来和NV交流的时候他们也直接建议我们不要去改他们GXF写的源码， 但是确实是有bug
做出目标估计我们需要验证精度，验证精度我们又是发难了，因为验证精度们，你总要找个能测出姿态的更精确的传感器吧，但是我们部门的支持也不够，所以我们也只能想其他办法，那怎么办呢，我们做视觉的，那只能想视觉的办法了。所以就用了Aruco标定版，solvepnp的方式去求解位姿。
结果方向foundation pose的姿态转化为欧拉角之后，和标定出来的有几度的差距，精度是达不到要求，foundationpose已经是深度学习最先进的方法了，我们的目标还是一个无纹理的物体，所以用colmap估计真实位姿也挺难，没办法制作数据集，learning的方法就不考虑了，只能想其他办法了。后来我们就把目标就投到了传统方法，就尝试用目标检测扣点云，ransac法向量估计，然后再将inlier点投影到平面上，去求最小外接转矩形，再把求的的方向反变换回来，得到物体真实姿态，然后发现传统方法确实要比foundation pose的效果好，就在我们的任务中用了最合适的传统方法。
点云孔洞处理 我们的插拔typeC操作需要近距离操作，所以最开始我们用了D405,因为官方文档不是说工具距离更近吗， 但是405是个双目，他对于无纹理和镜面深度估计不行，全是孔洞，最开始就是自己在物体背面涂画一些涂鸦，人为制造纹理，然后去打光，制造对比度，发现点云效果还行。但是实际产线肯定不让你涂东西的/ 后来我们只能改成435, 因为他是结构光 &#43; 双目，所以对于无纹理的可以很好的估计，但是我们发现效果有时候还是不行，点云时好时坏的，还是会出现大面积的孔洞。想尝试加偏振片，但是领导也不给予支持，所以也没办法。后来只能去开启滤波，包括孔洞填充，空间滤波，去让点云变得稳定。 但是滤波也不能解决核心问题，还是会有孔洞，因为我们从405的经验是打光能让点云变好，因为我们的物体是iphone手机么，两面都是高光材质，所以有镜面反射，所以我们猜测是不是镜面反射让结构光失效了，于是我们就把环境光调低，果然点云效果就变好了，然后解决了这个问题。 UV固化，一个月演示 时间特别紧，所有感知相关的活都要自己干，而且还要去深圳产线现场连调，一个月项目就要演示。
以前标数据都是阿里云去标，但是这次时间太紧了，阿里云标注时间特别长，所以我们就想借助大模型做工具，于是就去找了Grouded sam2, 利用他完成了5000张数据集的标注，利用ultralytics Yolov11进行训练，用BotSort完成多目标跟踪，3天实现产线上pad的稳定分割和追踪
工业相机的标定，一直不给买大的标定版，我们只能用A4纸打印的去标定，但是工业相机架的很高，所以手在眼外标定的精度很差，所以我们只能想其他迂回的办法了。 所幸的是，我们手臂上还有一个摄像头，我们手臂上眼在手上标定因为距离近，还是可以得到一个不错的精度的，用这个间接去算工业相机的标定。我们把标定版抬高，然后机械臂也抬高，知道标定版相对于工业相机关系，也知道标定版相对于手臂上realsense的关系，因为realsense的眼在手上有，这样就可以间接得到工业相机在baselink下的关系，最后为了结果稳定，我们将多次不同位置的结果做了平均。
最后pad放进固化炉子之后，因为需要用视觉方法确定其是否烟丝合缝的嵌入进去了，我们想的方法就是先走到固定点位，然后获取凹进去的平面的mask, 然后pad放进去之后，做完实例分割之后，二者求交，大于阈值就判断放好了，如果没有还需要二次调整，但是因为我们自研的机械臂重复定位精度差，沒办法每次走到相同点位，而且随着运行时间长越走越差，所以这种方法就失效了，为了能够做判断，我们只能是将相机固定在机械臂外，这样每次的凹进去的mask真实值就不会再变了。
yolov11 架构能讲解一下吗？但是我还是应该了解一下最后是怎么检测的
从我搞科研的经历说起，改模型是一项吃力不讨好的工作，收效甚微，对于我们做业务的人来说，能够快速上线，稳定检测是紧急的任务，所以我并不会优先去做改模型的工作。
我们的场景很单一，目标也比较单一, 所以我们更希望的是能够让灰度和彩色图像共用一套代码，我们对于数据预处理和数据增强做了些改进，比方说把灰度图叠加三个通道，使得其能和彩色图像一块训，然后比方说对于彩色图像数据，我们要随机让他转化为灰度图像，这样子去增强灰度图像检测效果。
VLA模型遇到的问题 末端姿态控制，导致经常进去奇异点。如果你判断他不懂了，你就要自己去做些短程的运动控制，去从奇异点走出来。 你好,.明天 Flow matching的原理 速度场建模，基于最速传输 初始化一个随即噪声和时间t, 然后在基于时间t在样本和噪声之间做插值，样本和噪声之间的最快速度就是应该二者的差值，去噪声过程就是去预测这两个差值， CVAE的原理- ACT的原理 encoder 基于observation 和 action 去预测隐变量的均值和方差">

    
    
    

</head><body><nav class="navbar is-light" role="navigation">
    <div class="container">
        <div class="navbar-brand">
            <a href="/" title="home" class="navbar-item">
                <span class="logo">
                    <h1>hello world ~~!</h1>
                </span>
            </a>

            
            <a id="theme-toggle" class="theme-toggle" href="#">
                <img src="https://summertaiyuan.github.io/svg/sun.svg" alt="sun icon" class="theme-icon" />
            </a>

            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>

        <div class="navbar-menu">
            <div class="navbar-start">
                
                <a href="/about" class="navbar-item">About</a>
                
                <a href="/post" class="navbar-item">Blog</a>
                
                <a href="/categories" class="navbar-item">Categories</a>
                
            </div>

        </div>
        <div class="search">
            <div id="fastSearch">
                <input id="searchInput" tabindex="0" placeholder="Search..">
                <ul id="searchResults">

                </ul>
            </div>
            <a id="search-btn" style="display: inline-block;" href="# ">
                <div class="icon-search"><svg class="search-svg" xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div>
            </a>
        </div>

        <script src="/js/fuse.min.js"></script> 
        <script src="/js/fastsearch.js"></script>

    </div>
</nav>

<script>
    
    document.addEventListener('DOMContentLoaded', function() {
        var burger = document.querySelector('.navbar-burger');
        burger.addEventListener('click', function() {
            burger.classList.toggle('is-active');
            document.querySelector('.navbar-menu').classList.toggle('is-active');
        });
    });

    
    function setTheme(theme) {
        let body = document.body;
        let themeIcon = document.querySelector(".theme-icon");
        if (theme === "dark") {
            body.classList.add("dark-mode");
            themeIcon.src = "https:\/\/summertaiyuan.github.io\/svg/moon.svg";
            themeIcon.alt = "moon icon";
        } else {
            body.classList.remove("dark-mode");
            themeIcon.src = "https:\/\/summertaiyuan.github.io\/svg/sun.svg";
            themeIcon.alt = "sun icon";
        }
        
        localStorage.setItem("theme", theme);
    }

    
    let theme = localStorage.getItem("theme") || "light";
    const isDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
    if (isDarkMode) {
        
        setTheme('dark');

    } else {
        
        setTheme('light');
    }
    setTheme(theme);

    
    document.getElementById("theme-toggle").addEventListener("click", function() {
        if (theme === "light") {
            theme = "dark";
        } else {
            theme = "light";
        }
        setTheme(theme);
    });



</script>

</header><main>
<div class="single-container">
    <div class="archive">
        <h1 class="title is-1">面试总结</h1>
        <div class="title subtitle heading is-6">
            <div class="author-info columns is-vcentered">
                <div class="column">
                    <div class="columns is-vcentered is-mobile">
                        
                        <div class="column is-narrow">
                            <img src="/images/avatar3.webp" class="author-image">
                        </div>
                        
                        <div class="column">
                            <p>greenwalnut</p>
                            <p><time>January 10, 2025</time>
                            </p>
                        </div>
                    </div>
                </div>
                <div class="small-categories-container">
                    
                </div>
            </div>
        </div>
        <div class="content article-content">
            <div class="toc-container">
                
    <div class="post-toc">
        
            <aside>
                <button id="tocButton" ><h4 id="contents" style="margin-left: 1vw;color:rgb(96, 134, 180);margin-bottom: 0;">CONTENTS</h4></button>
                <div id="hide"><nav id="TableOfContents">
  <ul>
    <li><a href="#位姿估计">位姿估计</a></li>
    <li><a href="#点云孔洞处理">点云孔洞处理</a></li>
    <li><a href="#uv固化一个月演示">UV固化，一个月演示</a></li>
  </ul>

  <ul>
    <li><a href="#flow-matching的原理">Flow matching的原理</a></li>
    <li><a href="#cvae的原理--act的原理">CVAE的原理- ACT的原理</a></li>
  </ul>
</nav></div>
            </aside>
        
    </div><script>
    
        let button = document.getElementById('tocButton');
        let hide = document.getElementById("hide");
        let contents=document.getElementById("contents");
        button.addEventListener("click", function() {
        if (hide.style.display!='block') {
            hide.style.display='block'
        } else {
            hide.style.display='none'
            contents.style.color='rgb(96, 134, 180)'
        }
        });
    




</script>
            </div>
            <h1 id="xunfei---自动驾驶">xunfei - 自动驾驶</h1>
<p>基本没做过相关工作，yolov11s也就是用的ultralytics, 没有怎么研究</p>
<p>唯一的收获就是他问我解决过什么技术难点，这块回答的太差了，明明做过很多工作，但是面试的时候啥也没想到，脑子简直蒙了</p>
<h2 id="位姿估计">位姿估计</h2>
<ol>
<li>
<p>我们试过目前最先进的方法了： 我们最初想做姿态估计的时候，肯定也是找目前的SOTA方法，就把注意力转向了foundationPose，， FoundationPose不仅有开源代码，还有在jetson平台上的Issac ROS Nitros加速代码，用的tensorrt 部署和GXF图框架，很符合我们的项目需求，所以我们就开始研究foundationPose.</p>
<p>先看pytorch源码，然后复现结果。foundationpose 其实方法特别暴力， 并且他只能做单目标的位姿估计，同时需要rgb, 深度图，合适的mesh（物体size不能差距太大）， 原论文采用了Nerf生成mesh的方法，但是Nerf重建方法对于小物体的精度不够，</p>
<p>我们发现Isaac Ros里面的Foundation Pose 的位置估计初始总是有偏差，然后导致随后的跟踪效果越来越差， 然后深刻理解他的原理之后， 我开始对GXF框架编写的FoundationPose代码进行debug, 然后发现是他们部署的代码有bug,  还有跟踪发出的时间戳保持了初始时刻姿态估计的时间戳，这是不符合实际项目的需求的，我们修复了他的源代码，满足了实际项目需求。</p>
<p><strong>因为GXF 是nvidia 内部使用的框架，所以网上公开的资料很少，开始我们只能通过点点资料 + 断点调试 + 猜测去分析整个项目，当时感觉很难很难继续往下推进，但是靠自己的努力，一点点的把bug啃了下来，后来和NV交流的时候他们也直接建议我们不要去改他们GXF写的源码， 但是确实是有bug</strong></p>
<p>做出目标估计我们需要验证精度，验证精度我们又是发难了，因为验证精度们，你总要找个能测出姿态的更精确的传感器吧，但是我们部门的支持也不够，所以我们也只能想其他办法，那怎么办呢，我们做视觉的，那只能想视觉的办法了。所以就用了Aruco标定版，solvepnp的方式去求解位姿。</p>
<p><strong>结果方向foundation pose的姿态转化为欧拉角之后，和标定出来的有几度的差距，精度是达不到要求，foundationpose已经是深度学习最先进的方法了，我们的目标还是一个无纹理的物体，所以用colmap估计真实位姿也挺难，没办法制作数据集，learning的方法就不考虑了，只能想其他办法了。后来我们就把目标就投到了传统方法，就尝试用目标检测扣点云，ransac法向量估计，然后再将inlier点投影到平面上，去求最小外接转矩形，再把求的的方向反变换回来，得到物体真实姿态，然后发现传统方法确实要比foundation pose的效果好，就在我们的任务中用了最合适的传统方法。</strong></p>
</li>
</ol>
<h2 id="点云孔洞处理">点云孔洞处理</h2>
<ol>
<li>我们的插拔typeC操作需要近距离操作，所以最开始我们用了D405,因为官方文档不是说工具距离更近吗， 但是405是个双目，他对于无纹理和镜面深度估计不行，全是孔洞，最开始就是自己在物体背面涂画一些涂鸦，人为制造纹理，然后去打光，制造对比度，发现点云效果还行。但是实际产线肯定不让你涂东西的/</li>
<li>后来我们只能改成435, 因为他是结构光 + 双目，所以对于无纹理的可以很好的估计，但是我们发现效果有时候还是不行，点云时好时坏的，还是会出现大面积的孔洞。想尝试加偏振片，但是领导也不给予支持，所以也没办法。后来只能去开启滤波，包括孔洞填充，空间滤波，去让点云变得稳定。</li>
<li>但是滤波也不能解决核心问题，还是会有孔洞，因为我们从405的经验是打光能让点云变好，因为我们的物体是iphone手机么，两面都是高光材质，所以有镜面反射，所以我们猜测是不是镜面反射让结构光失效了，于是我们就把环境光调低，果然点云效果就变好了，然后解决了这个问题。</li>
</ol>
<h2 id="uv固化一个月演示">UV固化，一个月演示</h2>
<ol>
<li>
<p>时间特别紧，所有感知相关的活都要自己干，而且还要去深圳产线现场连调，一个月项目就要演示。</p>
</li>
<li>
<p>以前标数据都是阿里云去标，但是这次时间太紧了，阿里云标注时间特别长，所以我们就想借助大模型做工具，于是就去找了Grouded sam2,   利用他完成了5000张数据集的标注，利用ultralytics Yolov11进行训练，用BotSort完成多目标跟踪，3天实现产线上pad的稳定分割和追踪</p>
</li>
<li>
<p>工业相机的标定，一直不给买大的标定版，我们只能用A4纸打印的去标定，但是工业相机架的很高，所以手在眼外标定的精度很差，所以我们只能想其他迂回的办法了。 所幸的是，我们手臂上还有一个摄像头，我们手臂上眼在手上标定因为距离近，还是可以得到一个不错的精度的，用这个间接去算工业相机的标定。我们把标定版抬高，然后机械臂也抬高，知道标定版相对于工业相机关系，也知道标定版相对于手臂上realsense的关系，因为realsense的眼在手上有，这样就可以间接得到工业相机在baselink下的关系，最后为了结果稳定，我们将多次不同位置的结果做了平均。</p>
</li>
<li>
<p>最后pad放进固化炉子之后，因为需要用视觉方法确定其是否烟丝合缝的嵌入进去了，我们想的方法就是先走到固定点位，然后获取凹进去的平面的mask, 然后pad放进去之后，做完实例分割之后，二者求交，大于阈值就判断放好了，如果没有还需要二次调整，但是因为我们自研的机械臂重复定位精度差，沒办法每次走到相同点位，而且随着运行时间长越走越差，所以这种方法就失效了，为了能够做判断，我们只能是将相机固定在机械臂外，这样每次的凹进去的mask真实值就不会再变了。</p>
</li>
</ol>
<p>yolov11 架构能讲解一下吗？<strong>但是我还是应该了解一下最后是怎么检测的</strong></p>
<blockquote>
<p>从我搞科研的经历说起，改模型是一项吃力不讨好的工作，收效甚微，对于我们做业务的人来说，能够快速上线，稳定检测是紧急的任务，所以我并不会优先去做改模型的工作。</p>
<p>我们的场景很单一，目标也比较单一, 所以我们更希望的是能够让灰度和彩色图像共用一套代码，我们对于数据预处理和数据增强做了些改进，比方说把灰度图叠加三个通道，使得其能和彩色图像一块训，然后比方说对于彩色图像数据，我们要随机让他转化为灰度图像，这样子去增强灰度图像检测效果。</p>
</blockquote>
<h1 id="vla模型遇到的问题">VLA模型遇到的问题</h1>
<ol>
<li>末端姿态控制，导致经常进去奇异点。如果你判断他不懂了，你就要自己去做些短程的运动控制，去从奇异点走出来。</li>
<li>你好,.明天</li>
</ol>
<h2 id="flow-matching的原理">Flow matching的原理</h2>
<ol>
<li>速度场建模，基于最速传输</li>
<li>初始化一个随即噪声和时间t,  然后在基于时间t在样本和噪声之间做插值，样本和噪声之间的最快速度就是应该二者的差值，去噪声过程就是去预测这两个差值，</li>
</ol>
<h2 id="cvae的原理--act的原理">CVAE的原理- ACT的原理</h2>
<ol>
<li>
<p>encoder 基于observation 和 action 去预测隐变量的均值和方差</p>
</li>
<li>
<p>decoder 基于 observation 和 隐变量Z生成的噪声去预测action. 基于重建误差和 标准正太分布的KL散度损失函数, 也就是优化ELBO(evidence lower bound) 下界</p>
</li>
<li></li>
</ol>
<h1 id="部署过程中的问题">部署过程中的问题</h1>
<ol>
<li>为了加速模型tensorrt推理的后处理过程，原本我们用的是numpy,但是我们profile之后发现后处理这部分代码耗时快赶上推理了，我们就开始优化，用cupy gpu矩阵计算去替代numpy，然后去加速后处理过程</li>
<li>优化点云姿态估计的速度，也是同样的思想，先去找热点，发现有点云生成和矩阵乘是比较大的热点，然后就是各种优化，合并操作，矩阵乘上gpu,  open3d 用cuda实现。</li>
<li>优化c++ tensorrt部署，主要是利用perf工具，去计算函数和代码行比较耗时，也是去尽量减少没有必要的操作。</li>
<li>还有很多工程和业务相关的事情，比如要比模型功能封装在ROS2节点中，而且还要保证能够同时处理多个请求操作，就是利用ROS2自己的多线程机制，最开始发生死锁问题， 一点一点的去debug</li>
</ol>

        </div>
    </div>
    <a href="#" id="scrollToTopButton">
        <svg t="1686753152588" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg"
            p-id="3988" width="48" height="48">
            <path
                d="M518.5 360.3c-3.2-4.4-9.7-4.4-12.9 0l-178 246c-3.8 5.3 0 12.7 6.5 12.7H381c10.2 0 19.9-4.9 25.9-13.2L512 460.4l105.2 145.4c6 8.3 15.6 13.2 25.9 13.2H690c6.5 0 10.3-7.4 6.5-12.7l-178-246z"
                p-id="3989" fill="#363636"></path>
            <path
                d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m0 820c-205.4 0-372-166.6-372-372s166.6-372 372-372 372 166.6 372 372-166.6 372-372 372z"
                p-id="3990" fill="#363636"></path>
        </svg>
    </a><hr style="border-top: 1px solid #EEEEEE;">
<div id="comment"></div>
<script>
    const getStoredTheme = () => localStorage.getItem("theme") === "dark" ? "dark" : "light";

    const setGiscusTheme = () => {
        const sendMessage = (message) => {
            const iframe = document.querySelector('iframe.giscus-frame');
            if (iframe) {
                iframe.contentWindow.postMessage({giscus: message}, 'https://giscus.app');
            }
        }
        sendMessage({setConfig: {theme: getStoredTheme()}})
    }

    document.addEventListener("DOMContentLoaded", () => {
        const giscusAttributes = {
            "src": "https://giscus.app/client.js",
            "data-repo": "hotjuicew\/blog",
            "data-repo-id": "R_kgDOHHQOuw",
            "data-category": "Announcements",
            "data-category-id": "DIC_kwDOHHQOu84CWCQ3",
            "data-mapping": "og:title",
            "data-reactions-enabled": "1",
            "data-emit-metadata": "0",
            "data-input-position": "bottom",
            "data-theme": getStoredTheme(),
            "data-lang": "en",
            "data-loading": "lazy",
            "crossorigin": "anonymous",
        };

        
        const giscusScript = document.createElement("script");
        Object.entries(giscusAttributes).forEach(
            ([key, value]) => giscusScript.setAttribute(key, value));
        document.getElementById("comment").appendChild(giscusScript);

        
        const themeToggle = document.querySelector(".theme-toggle");
        if (themeToggle) {
            themeToggle.addEventListener("click", setGiscusTheme);
        }
    });

</script>


<div class="pp-container">
        <section class="pre-and-post">
            <div class="has-text-left">
                
                <p>Previous post</p>
                <a href="https://summertaiyuan.github.io/post/helloworld/">linux 终端实用配置</a>
                
            </div>
            <div class="has-text-right">
                
                <p>Next post</p>
                <a href="https://summertaiyuan.github.io/post/transfrom/">Teleoperation</a>
                
            </div>
        </section>
    </div>

</div>

        </main><footer class="footer">
    <div class="content has-text-centered">
    <span>&copy; 2025 <a href="https://summertaiyuan.github.io/">hello world ~~!</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" target="_blank">Hugo</a> &
        <a href="https://github.com/hotjuicew/hugo-JuiceBar" target="_blank">JuiceBar</a>
    </span>
    </div>
  </footer></body>
</html>

<head>
    ...
    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    }
  };
</script>
    
    ...
</head>